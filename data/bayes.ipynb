{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建Bayes类\n",
    "class Bayes:\n",
    "    trainSet = None \n",
    "    labels = None\n",
    "    allWordsSet = None # 训练集中所有出现的单词\n",
    "    \n",
    "    # 先验情况下得到的概率\n",
    "    p0 = None\n",
    "    p1 = None\n",
    "    pClass1 = None\n",
    "    \n",
    "    def __inint__():\n",
    "        pass\n",
    "\n",
    "    # 格式化读取文件\n",
    "    def textParse(self,bigString):    #input is big string, #output is word list\n",
    "        import re\n",
    "        listOfTokens = re.split(r'\\W*', bigString)\n",
    "        return [tok.lower() for tok in listOfTokens if len(tok) > 2] \n",
    "\n",
    "    # 得到所有的单词\n",
    "    def createVocabList(self,dataSet):\n",
    "        allWordSet = set([])  #create empty set\n",
    "        for document in dataSet:\n",
    "#             print(document)\n",
    "            allWordSet = allWordSet | set(document) #union of the two sets\n",
    "        return list(allWordSet)\n",
    "    \n",
    "    \n",
    "    def loadDataSet(self):\n",
    "        trainSet= [] # 前面是训练数据，后面是lable\n",
    "        labels = []\n",
    "        #读取所有的ham文件,文件名字已经格式化为i.ham.txt\n",
    "        for i in range(1,3014):\n",
    "            if os.path.isfile('ham/'+str(i)+'.ham.txt') == False: #没有这个路径\n",
    "                continue\n",
    "            try:\n",
    "                wordList = self.textParse(open('ham/'+str(i)+'.ham.txt').read())\n",
    "                trainSet.append(wordList)\n",
    "                labels.append(1)\n",
    "    #         print(i)\n",
    "            except UnicodeEncodeError:\n",
    "                pass\n",
    "\n",
    "        #读取所有的spam文件,文件名字已经格式化为i.spam.txt\n",
    "        for i in range(1,1127):\n",
    "            if os.path.isfile('spam/'+str(i)+'.spam.txt') == False: #没有这个路径\n",
    "                continue\n",
    "            try:\n",
    "\n",
    "                wordList = self.textParse(open('spam/'+str(i)+'.spam.txt').read())\n",
    "                trainSet.append(wordList)\n",
    "                labels.append(0)\n",
    "    #             print(i)\n",
    "            except UnicodeDecodeError:\n",
    "    #             os.remove('spam/'+str(i)+'.spam.txt')\n",
    "                pass\n",
    "            \n",
    "        # 得到训练集合和所有单词\n",
    "        self.trainSet = trainSet\n",
    "        self.labels = labels\n",
    "        self.allWordsSet = self.createVocabList(trainSet)\n",
    "        \n",
    "        \n",
    "        return trainSet,labels\n",
    "    \n",
    "    # 词袋模型\n",
    "    def bagOfWords2Vec(self,inputSet):\n",
    "        returnVec = [0 for i in range(0,len(self.allWordsSet))]\n",
    "        \n",
    "        for word in inputSet:\n",
    "            if word in self.allWordsSet:\n",
    "                returnVec[self.allWordsSet.index(word)] += 1\n",
    "\n",
    "        return returnVec\n",
    "\n",
    "    # 训练分类器\n",
    "    def fit(self,trainData,trainCategory):\n",
    "        # 得到所有单词\n",
    "        self.allWordsSet = self.createVocabList(trainData)\n",
    "        \n",
    "        \n",
    "        trainMatrix = []\n",
    "        #这儿最费时，\n",
    "        for doc in trainData:\n",
    "            trainMatrix.append(self.bagOfWords2Vec(doc))\n",
    "\n",
    "        numTrainDocs = len(trainMatrix)\n",
    "        numWords = len(trainMatrix[0])\n",
    "        \n",
    "        pAb = sum(trainCategory) / float(numTrainDocs)\n",
    "        p0 = ones(numWords); p1 = ones(numWords)      #所有次初始化1，\n",
    "        p0Denom = 2.0; p1Denom = 2.0                        #分母初始化2\n",
    " \n",
    "        for i in range(numTrainDocs):\n",
    "            if trainCategory[i] == 1:\n",
    "                p1 += trainMatrix[i]\n",
    "                p1Denom += sum(trainMatrix[i])\n",
    "            else:\n",
    "                p0 += trainMatrix[i]\n",
    "                p0Denom += sum(trainMatrix[i])\n",
    "        p1V = log(p1 / p1Denom)          #change to log()\n",
    "        p0V = log(p0 / p0Denom)          #change to log()\n",
    "        \n",
    "        self.p0 = p0V\n",
    "        self.p1 = p1V\n",
    "        self.pClass1 = pAb\n",
    "        \n",
    "        return p0V,p1V,pAb\n",
    "    \n",
    "    # 预测X,X是个Numpy列表，或者是个文件名\n",
    "    def predict(self,X):\n",
    "        if type(X) != list and os.path.isfile(X): #X是个文件名,转化为numpy列表\n",
    "            X = self.textParse(open(X).read())\n",
    "        \n",
    "#         print(X)\n",
    "        # 转化为概率\n",
    "        px = self.bagOfWords2Vec(X)\n",
    "#         print(px)\n",
    "        p1 = sum(px * self.p1) + log(self.pClass1)    #element-wise mult\n",
    "        p0 = sum(px * self.p0) + log(1.0 - self.pClass1)\n",
    "        if p1 > p0:\n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def predictall(self, testSet):\n",
    "        res = []\n",
    "        for x in testSet:\n",
    "            res.append(self.predict(x))\n",
    "            \n",
    "        return res\n",
    "        \n",
    "    \n",
    "    # k折交叉得到的数据，\n",
    "    def kfold(self,k,seed=0):\n",
    "        '''\n",
    "        k分数据，seed是一个小于k的值，得到的第seed份数据进行测试，其余作为训练\n",
    "        '''\n",
    "        from random import shuffle # 用来打乱数据\n",
    "        length = len(self.trainSet)\n",
    "        sign = length // k\n",
    "\n",
    "        trainSetk = []\n",
    "        labelsk = []\n",
    "        index = [i for i in range(length)] #得到所欲数据的下标，打乱下标，之后讲数据等分成k分\n",
    "        shuffle(index)\n",
    "        vis = [0 for i in range(k)]\n",
    "#         print(vis)\n",
    "        vis[0] = 1\n",
    "        tmp1 = []       \n",
    "        tmp2 = []\n",
    "\n",
    "        for i in range(length-length%k):\n",
    "    #         print(i // sign)\n",
    "            if vis[i // sign] == 0:\n",
    "                vis[i // sign] = 1\n",
    "                trainSetk.append(tmp1)\n",
    "                labelsk.append(tmp2)\n",
    "                tmp1 = []\n",
    "                tmp2 = []\n",
    "\n",
    "            tmp1.append(self.trainSet[index[i]])\n",
    "            tmp2.append(self.labels[index[i]])\n",
    "            \n",
    "        #得到最后的数据\n",
    "        trainSetk.append(tmp1)\n",
    "        labelsk.append(tmp2)\n",
    "        \n",
    "#         for i in range(k):\n",
    "#             print(len(trainSetk[i]))\n",
    "        \n",
    "        newtrinSet = []\n",
    "        newtrainSet_labels = []\n",
    "        \n",
    "        for i in range(k):\n",
    "            if i != seed:\n",
    "                newtrinSet.extend(trainSetk[i])\n",
    "                newtrainSet_labels.extend(labelsk[i])\n",
    "        \n",
    "        test = trainSetk[seed]\n",
    "        testlabels = labelsk[seed]\n",
    "        \n",
    "        return newtrinSet, newtrainSet_labels, test, testlabels\n",
    "    \n",
    "    \n",
    "    def culprecise(self,predict_res, test_labels):\n",
    "        '''\n",
    "        计算正确率\n",
    "        ''' \n",
    "        return sum(array(predict_res) == array(test_labels)) / len(test_labels)\n",
    "        \n",
    "        \n",
    "    def culrecall(self,predict_res, test_labels):\n",
    "        '''\n",
    "        计算召回率\n",
    "        '''\n",
    "        k = 0\n",
    "        for i in predict_res:\n",
    "            if i in test_labels:\n",
    "                k += 1\n",
    "        \n",
    "        return k / len(test_labels)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bayes = Bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoran/anaconda3/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "trainSet,labels = bayes.loadDataSet()\n",
    "train,train_labels, test, test_labels = bayes.kfold(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-7.56008047, -7.56008047, -6.86693328, ..., -7.56008047,\n",
       "        -7.56008047, -7.56008047]),\n",
       " array([-8.29125407, -8.98440125, -8.47357563, ..., -9.38986636,\n",
       "        -8.003572  , -9.38986636]),\n",
       " 0.81000000000000005)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes.fit(train[:100],train_labels[:100]) #训练时间费时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoran/anaconda3/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes.predict('spam/1.spam.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = bayes.predictall(test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preciserate = bayes.culprecise(ans,test_labels[:100])\n",
    "recallrate = bayes.culrecall(ans,test_labels[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preciserate= 0.34\n",
      "recallrate= 1.0\n"
     ]
    }
   ],
   "source": [
    "print('preciserate=',preciserate)\n",
    "print('recallrate=',recallrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
